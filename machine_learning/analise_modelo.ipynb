{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üìä An√°lise Completa do Modelo UNDER/OVER Total Kills\n",
        "\n",
        "Este notebook cont√©m an√°lise completa do modelo de ML para prever se total_kills ser√° OVER ou UNDER da m√©dia da liga."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Prepara√ß√£o dos Dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preparando dados...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Exception in thread Thread-3 (_readerthread):\n",
            "Traceback (most recent call last):\n",
            "  File \u001b[35m\"C:\\Users\\Matheus\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\threading.py\"\u001b[0m, line \u001b[35m1082\u001b[0m, in \u001b[35m_bootstrap_inner\u001b[0m\n",
            "    \u001b[31mself._context.run\u001b[0m\u001b[1;31m(self.run)\u001b[0m\n",
            "    \u001b[31m~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^\u001b[0m\n",
            "  File \u001b[35m\"C:\\Users\\Matheus\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\threading.py\"\u001b[0m, line \u001b[35m1024\u001b[0m, in \u001b[35mrun\u001b[0m\n",
            "    \u001b[31mself._target\u001b[0m\u001b[1;31m(*self._args, **self._kwargs)\u001b[0m\n",
            "    \u001b[31m~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
            "  File \u001b[35m\"C:\\Users\\Matheus\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\subprocess.py\"\u001b[0m, line \u001b[35m1613\u001b[0m, in \u001b[35m_readerthread\u001b[0m\n",
            "    buffer.append(\u001b[31mfh.read\u001b[0m\u001b[1;31m()\u001b[0m)\n",
            "                  \u001b[31m~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
            "  File \u001b[35m\"C:\\Users\\Matheus\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\encodings\\cp1252.py\"\u001b[0m, line \u001b[35m23\u001b[0m, in \u001b[35mdecode\u001b[0m\n",
            "    return \u001b[31mcodecs.charmap_decode\u001b[0m\u001b[1;31m(input,self.errors,decoding_table)\u001b[0m[0]\n",
            "           \u001b[31m~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
            "\u001b[1;35mUnicodeDecodeError\u001b[0m: \u001b[35m'charmap' codec can't decode byte 0x8d in position 1426: character maps to <undefined>\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "None\n"
          ]
        }
      ],
      "source": [
        "# Prepara dados diretamente (importa o m√≥dulo ao inv√©s de subprocess)\n",
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Ajusta path para importar m√≥dulos\n",
        "current_dir = Path.cwd()\n",
        "# Se n√£o estiver no diret√≥rio machine_learning, tenta encontrar\n",
        "if not (current_dir / \"data_preparation.py\").exists():\n",
        "    possible_dirs = [\n",
        "        current_dir / \"machine_learning\",\n",
        "        current_dir.parent / \"machine_learning\"\n",
        "    ]\n",
        "    for ml_dir in possible_dirs:\n",
        "        if (ml_dir / \"data_preparation.py\").exists():\n",
        "            os.chdir(ml_dir)\n",
        "            sys.path.insert(0, str(ml_dir))\n",
        "            break\n",
        "\n",
        "print(\"Preparando dados...\")\n",
        "try:\n",
        "    # Importa e executa diretamente\n",
        "    from data_preparation import main as prep_main\n",
        "    prep_main()\n",
        "    print(\"Dados preparados com sucesso!\")\n",
        "except Exception as e:\n",
        "    print(f\"Erro ao preparar dados: {e}\")\n",
        "    print(\"Continuando com dados existentes (se j√° foram preparados)...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Carregamento dos Dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": []
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "from pathlib import Path\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Tenta importar matplotlib e seaborn, se n√£o estiver instalado, instala\n",
        "try:\n",
        "    import matplotlib.pyplot as plt\n",
        "    import seaborn as sns\n",
        "except ImportError:\n",
        "    print(\"Instalando matplotlib e seaborn...\")\n",
        "    import subprocess\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"matplotlib\", \"seaborn\", \"-q\", \"--quiet\"])\n",
        "    import matplotlib.pyplot as plt\n",
        "    import seaborn as sns\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    roc_curve, auc, precision_recall_curve, average_precision_score,\n",
        "    confusion_matrix, classification_report, f1_score, precision_score, recall_score\n",
        ")\n",
        "\n",
        "# Configura√ß√£o\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "\n",
        "# Ajusta diret√≥rio de trabalho - garante que estamos no diret√≥rio correto\n",
        "current_dir = Path.cwd()\n",
        "if not (current_dir / \"data_preparation.py\").exists():\n",
        "    # Tenta encontrar o diret√≥rio machine_learning\n",
        "    possible_paths = [\n",
        "        current_dir / \"machine_learning\",\n",
        "        current_dir.parent / \"machine_learning\"\n",
        "    ]\n",
        "    for path in possible_paths:\n",
        "        if path.exists() and (path / \"data_preparation.py\").exists():\n",
        "            os.chdir(path)\n",
        "            current_dir = path\n",
        "            break\n",
        "\n",
        "DATA_DIR = current_dir / \"data\"\n",
        "\n",
        "# Carrega dados\n",
        "features_df = pd.read_csv(DATA_DIR / \"features.csv\")\n",
        "labels = np.load(DATA_DIR / \"labels.npy\")\n",
        "\n",
        "with open(DATA_DIR / \"league_stats.pkl\", \"rb\") as f:\n",
        "    league_stats = pickle.load(f)\n",
        "\n",
        "print(f\"Features shape: {features_df.shape}\")\n",
        "print(f\"Labels shape: {labels.shape}\")\n",
        "print(f\"\\nDistribuicao de labels:\")\n",
        "print(f\"  UNDER (0): {np.sum(labels == 0)} ({np.sum(labels == 0)/len(labels)*100:.1f}%)\")\n",
        "print(f\"  OVER (1): {np.sum(labels == 1)} ({np.sum(labels == 1)/len(labels)*100:.1f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. An√°lise Explorat√≥ria"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Estat√≠sticas por liga\n",
        "print(\"Estat√≠sticas por liga:\")\n",
        "for league in sorted(league_stats.keys()):\n",
        "    stats = league_stats[league]\n",
        "    print(f\"  {league:8s}: m√©dia={stats['mean']:5.2f}, std={stats['std']:5.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualiza√ß√£o das m√©dias por liga\n",
        "leagues = sorted(league_stats.keys())\n",
        "means = [league_stats[lg]['mean'] for lg in leagues]\n",
        "stds = [league_stats[lg]['std'] for lg in leagues]\n",
        "\n",
        "plt.figure(figsize=(14, 6))\n",
        "plt.bar(leagues, means, yerr=stds, capsize=5, alpha=0.7, color='steelblue')\n",
        "plt.xlabel('Liga', fontsize=12)\n",
        "plt.ylabel('M√©dia de Total Kills', fontsize=12)\n",
        "plt.title('M√©dia de Total Kills por Liga', fontsize=14, fontweight='bold')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.grid(axis='y', alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nVaria√ß√£o entre ligas: {max(means) - min(means):.2f} kills\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Treinamento do Modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Prepara dados\n",
        "X = features_df.values\n",
        "y = labels\n",
        "\n",
        "# Split train/test - guarda √≠ndices para an√°lise posterior\n",
        "indices = np.arange(len(X))\n",
        "X_train, X_test, y_train, y_test, train_idx, test_idx = train_test_split(\n",
        "    X, y, indices, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"Train: {len(X_train)} amostras\")\n",
        "print(f\"Test: {len(X_test)} amostras\")\n",
        "print(f\"\\nDistribui√ß√£o train: UNDER={np.sum(y_train == 0)}, OVER={np.sum(y_train == 1)}\")\n",
        "print(f\"Distribui√ß√£o test: UNDER={np.sum(y_test == 0)}, OVER={np.sum(y_test == 1)}\")\n",
        "print(f\"\\n√çndices salvos: train_idx (primeiros 5)={train_idx[:5]}, test_idx (primeiros 5)={test_idx[:5]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Normaliza√ß√£o\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Treina modelo\n",
        "model = LogisticRegression(\n",
        "    max_iter=1000,\n",
        "    random_state=42,\n",
        "    class_weight='balanced'\n",
        ")\n",
        "\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predi√ß√µes\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "print(\"Modelo treinado com sucesso!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. M√©tricas de Performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, roc_auc_score\n",
        "\n",
        "# M√©tricas b√°sicas\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"M√âTRICAS DO MODELO\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\nAccuracy:  {accuracy:.4f}\")\n",
        "print(f\"ROC-AUC:    {roc_auc:.4f}\")\n",
        "print(f\"Precision:  {precision:.4f}\")\n",
        "print(f\"Recall:     {recall:.4f}\")\n",
        "print(f\"F1-Score:   {f1:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Classification Report detalhado\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"CLASSIFICATION REPORT\")\n",
        "print(\"=\" * 60)\n",
        "print(classification_report(y_test, y_pred, target_names=['UNDER', 'OVER']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Curva ROC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calcula curva ROC\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "# Plota curva ROC\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate', fontsize=12)\n",
        "plt.ylabel('True Positive Rate', fontsize=12)\n",
        "plt.title('ROC Curve - Modelo UNDER/OVER Total Kills', fontsize=14, fontweight='bold')\n",
        "plt.legend(loc=\"lower right\", fontsize=11)\n",
        "plt.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nAUC-ROC: {roc_auc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Precision-Recall Curve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calcula Precision-Recall curve\n",
        "precision_curve, recall_curve, pr_thresholds = precision_recall_curve(y_test, y_pred_proba)\n",
        "avg_precision = average_precision_score(y_test, y_pred_proba)\n",
        "\n",
        "# Plota Precision-Recall curve\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.plot(recall_curve, precision_curve, color='darkblue', lw=2, \n",
        "         label=f'PR curve (AP = {avg_precision:.4f})')\n",
        "plt.xlabel('Recall', fontsize=12)\n",
        "plt.ylabel('Precision', fontsize=12)\n",
        "plt.title('Precision-Recall Curve', fontsize=14, fontweight='bold')\n",
        "plt.legend(loc=\"lower left\", fontsize=11)\n",
        "plt.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nAverage Precision: {avg_precision:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. F1-Score por Threshold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calcula F1 para diferentes thresholds\n",
        "thresholds_range = np.arange(0.1, 1.0, 0.05)\n",
        "f1_scores = []\n",
        "\n",
        "for threshold in thresholds_range:\n",
        "    y_pred_thresh = (y_pred_proba >= threshold).astype(int)\n",
        "    f1 = f1_score(y_test, y_pred_thresh)\n",
        "    f1_scores.append(f1)\n",
        "\n",
        "# Plota F1 por threshold\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(thresholds_range, f1_scores, marker='o', color='green', lw=2)\n",
        "plt.xlabel('Threshold', fontsize=12)\n",
        "plt.ylabel('F1-Score', fontsize=12)\n",
        "plt.title('F1-Score por Threshold', fontsize=14, fontweight='bold')\n",
        "plt.grid(alpha=0.3)\n",
        "plt.axvline(x=0.5, color='red', linestyle='--', label='Threshold padr√£o (0.5)')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Melhor threshold\n",
        "best_idx = np.argmax(f1_scores)\n",
        "best_threshold = thresholds_range[best_idx]\n",
        "best_f1 = f1_scores[best_idx]\n",
        "print(f\"\\nMelhor threshold: {best_threshold:.2f} (F1 = {best_f1:.4f})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Plota confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "            xticklabels=['UNDER', 'OVER'], \n",
        "            yticklabels=['UNDER', 'OVER'])\n",
        "plt.ylabel('True Label', fontsize=12)\n",
        "plt.xlabel('Predicted Label', fontsize=12)\n",
        "plt.title('Confusion Matrix', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# M√©tricas da confusion matrix\n",
        "tn, fp, fn, tp = cm.ravel()\n",
        "print(f\"\\nTrue Negatives (UNDER predito corretamente):  {tn}\")\n",
        "print(f\"False Positives (OVER predito incorretamente): {fp}\")\n",
        "print(f\"False Negatives (UNDER predito incorretamente): {fn}\")\n",
        "print(f\"True Positives (OVER predito corretamente):     {tp}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Distribui√ß√£o de Probabilidades"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Histograma de probabilidades\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.hist(y_pred_proba[y_test == 0], bins=20, alpha=0.7, label='UNDER (True)', color='red')\n",
        "plt.hist(y_pred_proba[y_test == 1], bins=20, alpha=0.7, label='OVER (True)', color='green')\n",
        "plt.xlabel('Probabilidade Predita (OVER)', fontsize=11)\n",
        "plt.ylabel('Frequ√™ncia', fontsize=11)\n",
        "plt.title('Distribui√ß√£o de Probabilidades', fontsize=12, fontweight='bold')\n",
        "plt.legend()\n",
        "plt.grid(alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.boxplot([y_pred_proba[y_test == 0], y_pred_proba[y_test == 1]], \n",
        "            labels=['UNDER', 'OVER'])\n",
        "plt.ylabel('Probabilidade Predita (OVER)', fontsize=11)\n",
        "plt.title('Boxplot de Probabilidades', fontsize=12, fontweight='bold')\n",
        "plt.grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. An√°lise por Liga"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Carrega dados originais para an√°lise por liga\n",
        "df_original = pd.read_csv(\"../database_improved/data_transformed.csv\")\n",
        "\n",
        "# Cria dataframe de an√°lise usando os √≠ndices do test set\n",
        "# test_idx foi salvo na c√©lula anterior\n",
        "df_analysis = pd.DataFrame(index=test_idx)\n",
        "df_analysis['y_test'] = y_test\n",
        "df_analysis['y_pred'] = y_pred\n",
        "df_analysis['y_pred_proba'] = y_pred_proba\n",
        "\n",
        "# Adiciona informa√ß√µes da liga do dataframe original\n",
        "# Alinha pelos √≠ndices do dataframe original\n",
        "df_analysis['league'] = df_original.loc[test_idx, 'league'].values\n",
        "\n",
        "# M√©tricas por liga (apenas test set)\n",
        "print(\"M√©tricas por Liga (Test Set):\")\n",
        "print(\"=\" * 80)\n",
        "for league in sorted(df_analysis['league'].unique()):\n",
        "    league_data = df_analysis[df_analysis['league'] == league]\n",
        "    if len(league_data) > 0:\n",
        "        league_y_test = league_data['y_test'].values\n",
        "        league_y_pred = league_data['y_pred'].values\n",
        "        \n",
        "        if len(np.unique(league_y_test)) > 1:  # Precisa ter ambas as classes\n",
        "            acc = accuracy_score(league_y_test, league_y_pred)\n",
        "            f1 = f1_score(league_y_test, league_y_pred)\n",
        "            print(f\"{league:8s}: Accuracy={acc:.3f}, F1={f1:.3f}, Amostras={len(league_data)}\")\n",
        "        elif len(league_data) > 0:\n",
        "            # Liga com apenas uma classe no test set\n",
        "            acc = accuracy_score(league_y_test, league_y_pred)\n",
        "            print(f\"{league:8s}: Accuracy={acc:.3f}, Amostras={len(league_data)} (apenas uma classe)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Teste de Predi√ß√£o"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Usa o modelo j√° treinado nesta sess√£o\n",
        "# Carrega dados necess√°rios\n",
        "with open(DATA_DIR / \"champion_impacts.pkl\", \"rb\") as f:\n",
        "    champion_impacts_loaded = pickle.load(f)\n",
        "league_stats_loaded = league_stats\n",
        "feature_columns_loaded = list(features_df.columns)\n",
        "\n",
        "# Fun√ß√£o auxiliar para criar features (mesma l√≥gica do predict.py)\n",
        "def create_features_from_game_local(game_data, league_stats, champion_impacts, feature_columns):\n",
        "    league = game_data['league']\n",
        "    league_impacts = champion_impacts.get(league, {})\n",
        "    \n",
        "    def normalize_champ(champ):\n",
        "        if not champ:\n",
        "            return ''\n",
        "        return str(champ).strip()\n",
        "    \n",
        "    top_t1_impact = league_impacts.get(normalize_champ(game_data.get('top_t1', '')), 0.0)\n",
        "    jung_t1_impact = league_impacts.get(normalize_champ(game_data.get('jung_t1', '')), 0.0)\n",
        "    mid_t1_impact = league_impacts.get(normalize_champ(game_data.get('mid_t1', '')), 0.0)\n",
        "    adc_t1_impact = league_impacts.get(normalize_champ(game_data.get('adc_t1', '')), 0.0)\n",
        "    sup_t1_impact = league_impacts.get(normalize_champ(game_data.get('sup_t1', '')), 0.0)\n",
        "    \n",
        "    top_t2_impact = league_impacts.get(normalize_champ(game_data.get('top_t2', '')), 0.0)\n",
        "    jung_t2_impact = league_impacts.get(normalize_champ(game_data.get('jung_t2', '')), 0.0)\n",
        "    mid_t2_impact = league_impacts.get(normalize_champ(game_data.get('mid_t2', '')), 0.0)\n",
        "    adc_t2_impact = league_impacts.get(normalize_champ(game_data.get('adc_t2', '')), 0.0)\n",
        "    sup_t2_impact = league_impacts.get(normalize_champ(game_data.get('sup_t2', '')), 0.0)\n",
        "    \n",
        "    team1_avg_impact = np.mean([top_t1_impact, jung_t1_impact, mid_t1_impact, adc_t1_impact, sup_t1_impact])\n",
        "    team2_avg_impact = np.mean([top_t2_impact, jung_t2_impact, mid_t2_impact, adc_t2_impact, sup_t2_impact])\n",
        "    impact_diff = team1_avg_impact - team2_avg_impact\n",
        "    \n",
        "    league_mean = league_stats.get(league, {}).get('mean', 0.0)\n",
        "    league_std = league_stats.get(league, {}).get('std', 0.0)\n",
        "    \n",
        "    feature_dict = {\n",
        "        'league_mean': league_mean,\n",
        "        'league_std': league_std,\n",
        "        'team1_avg_impact': team1_avg_impact,\n",
        "        'team2_avg_impact': team2_avg_impact,\n",
        "        'impact_diff': impact_diff,\n",
        "        'top_t1_impact': top_t1_impact,\n",
        "        'jung_t1_impact': jung_t1_impact,\n",
        "        'mid_t1_impact': mid_t1_impact,\n",
        "        'adc_t1_impact': adc_t1_impact,\n",
        "        'sup_t1_impact': sup_t1_impact,\n",
        "        'top_t2_impact': top_t2_impact,\n",
        "        'jung_t2_impact': jung_t2_impact,\n",
        "        'mid_t2_impact': mid_t2_impact,\n",
        "        'adc_t2_impact': adc_t2_impact,\n",
        "        'sup_t2_impact': sup_t2_impact,\n",
        "    }\n",
        "    \n",
        "    for col in feature_columns:\n",
        "        if col.startswith('league_') and col != 'league_mean' and col != 'league_std':\n",
        "            liga_name = col.replace('league_', '')\n",
        "            feature_dict[col] = 1.0 if liga_name == league else 0.0\n",
        "    \n",
        "    features = np.array([feature_dict.get(col, 0.0) for col in feature_columns])\n",
        "    return features.reshape(1, -1)\n",
        "\n",
        "# Exemplo de jogo\n",
        "game_example = {\n",
        "    'league': 'LCK',\n",
        "    'top_t1': 'Aatrox',\n",
        "    'jung_t1': 'Graves',\n",
        "    'mid_t1': 'Azir',\n",
        "    'adc_t1': 'Jinx',\n",
        "    'sup_t1': 'Thresh',\n",
        "    'top_t2': 'Gnar',\n",
        "    'jung_t2': 'Sejuani',\n",
        "    'mid_t2': 'Orianna',\n",
        "    'adc_t2': 'Aphelios',\n",
        "    'sup_t2': 'Braum'\n",
        "}\n",
        "\n",
        "# Predi√ß√£o para m√©dia da liga usando modelo da sess√£o\n",
        "X_game = create_features_from_game_local(game_example, league_stats_loaded, \n",
        "                                         champion_impacts_loaded, feature_columns_loaded)\n",
        "X_game_scaled = scaler.transform(X_game)\n",
        "prob_over = model.predict_proba(X_game_scaled)[0, 1]\n",
        "\n",
        "pred_mean = {\n",
        "    'league_mean': league_stats_loaded.get(game_example['league'], {}).get('mean', 0.0),\n",
        "    'probability_over_mean': prob_over,\n",
        "    'probability_under_mean': 1 - prob_over,\n",
        "    'prediction': 'OVER' if prob_over >= 0.5 else 'UNDER',\n",
        "    'confidence': 'High' if prob_over >= 0.70 or prob_over <= 0.30 else 'Medium'\n",
        "}\n",
        "\n",
        "print(\"Exemplo de Predi√ß√£o:\")\n",
        "print(f\"Liga: {game_example['league']}\")\n",
        "print(f\"M√©dia da liga: {pred_mean['league_mean']:.2f} kills\")\n",
        "print(f\"Probabilidade OVER m√©dia: {pred_mean['probability_over_mean']:.1%}\")\n",
        "print(f\"Predi√ß√£o: {pred_mean['prediction']} (Confian√ßa: {pred_mean['confidence']})\")\n",
        "\n",
        "# Predi√ß√£o para linha espec√≠fica\n",
        "betting_line = 28.5\n",
        "league_mean = league_stats_loaded.get(game_example['league'], {}).get('mean', 0.0)\n",
        "league_std = league_stats_loaded.get(game_example['league'], {}).get('std', 1.0)\n",
        "prob_over_mean = pred_mean['probability_over_mean']\n",
        "\n",
        "if league_std > 0:\n",
        "    z_score = (betting_line - league_mean) / league_std\n",
        "    adjustment = 1 / (1 + np.exp(-z_score * 0.5))\n",
        "    if betting_line > league_mean:\n",
        "        prob_over_line = prob_over_mean * (1 - adjustment * 0.3)\n",
        "    else:\n",
        "        prob_over_line = prob_over_mean + (1 - prob_over_mean) * adjustment * 0.3\n",
        "    prob_over_line = np.clip(prob_over_line, 0.0, 1.0)\n",
        "else:\n",
        "    prob_over_line = prob_over_mean\n",
        "\n",
        "pred_line = {\n",
        "    'probability_over_line': prob_over_line,\n",
        "    'probability_under_line': 1 - prob_over_line,\n",
        "    'bet_over': prob_over_line >= 0.55,\n",
        "    'bet_under': (1 - prob_over_line) >= 0.55\n",
        "}\n",
        "\n",
        "print(f\"\\nPara linha da casa {betting_line}:\")\n",
        "print(f\"Probabilidade OVER {betting_line}: {pred_line['probability_over_line']:.1%}\")\n",
        "if pred_line['bet_over']:\n",
        "    print(f\"Recomenda√ß√£o: APOSTAR OVER {betting_line}\")\n",
        "elif pred_line['bet_under']:\n",
        "    print(f\"Recomenda√ß√£o: APOSTAR UNDER {betting_line}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. Resumo Final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"RESUMO FINAL DO MODELO\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\nDataset:\")\n",
        "print(f\"  Total de amostras: {len(features_df)}\")\n",
        "print(f\"  Train: {len(X_train)}\")\n",
        "print(f\"  Test: {len(X_test)}\")\n",
        "print(f\"\\nPerformance:\")\n",
        "print(f\"  Accuracy:  {accuracy:.4f}\")\n",
        "print(f\"  ROC-AUC:    {roc_auc:.4f}\")\n",
        "print(f\"  Precision:  {precision:.4f}\")\n",
        "print(f\"  Recall:     {recall:.4f}\")\n",
        "print(f\"  F1-Score:   {f1:.4f}\")\n",
        "\n",
        "# Verifica se best_threshold foi definido\n",
        "try:\n",
        "    print(f\"\\nMelhor Threshold: {best_threshold:.2f} (F1 = {best_f1:.4f})\")\n",
        "except NameError:\n",
        "    print(f\"\\nMelhor Threshold: 0.50 (padr√£o)\")\n",
        "\n",
        "print(f\"\\nModelo treinado com sucesso e pronto para uso!\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
